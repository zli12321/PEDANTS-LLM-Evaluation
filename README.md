# QA-Evaluation-Metrics ðŸ“Š

[![PyPI version qa-metrics](https://img.shields.io/pypi/v/qa-metrics.svg)](https://pypi.org/project/qa-metrics/) 
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Ke23KIeHFdPWad0BModmcWKZ6jSbF5nI?usp=sharing)

> Check out the main [Repo](https://github.com/zli12321/qa_metrics)

> `pip install qa-metrics` is all you need!

> A fast and lightweight Python package for evaluating question-answering models and prompting of black-box and open-source large language models.

> ðŸ¤— Huggingface [Model](https://huggingface.co/zli12321/roberta-large-qa-evaluator) and [Dataset](https://huggingface.co/datasets/zli12321/pedants_qa_evaluation_bench)

